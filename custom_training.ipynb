{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPOijNtRqjVqp6Vf9sjNW8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitchelledema/mitche/blob/main/custom_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Training Setup\n",
        "Full guide [here](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/migration/UJ3%20Vertex%20SDK%20Custom%20Image%20Classification%20with%20custom%20training%20container.ipynb)"
      ],
      "metadata": {
        "id": "uwimklZXHrEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup environment variables"
      ],
      "metadata": {
        "id": "nflrmovEHw91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! gcloud auth login"
      ],
      "metadata": {
        "id": "jCDo9JiOMoOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newInstance = False\n",
        "\n",
        "REGION = \"us-west2\"\n",
        "PROJECT_ID = \"calcium-bot-373723\"\n",
        "BUCKET_NAME = \"custom_model2\"\n",
        "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
        "\n",
        "! gcloud config set project $PROJECT_ID"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUcO8BwBHsfP",
        "outputId": "7fc70302-e5e9-476b-9a96-650bb8fc14c0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up trainer requirements"
      ],
      "metadata": {
        "id": "RNbP0NXUJlNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make folder for Python training script\n",
        "! rm -rf custom\n",
        "! mkdir custom\n",
        "\n",
        "# Add package information\n",
        "! touch custom/README.md\n",
        "\n",
        "setup_cfg = \"[egg_info]\\ntag_build = tag_date = 0\"\n",
        "! echo \"$setup_cfg\" > custom/setup.cfg\n",
        "\n",
        "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n  install_requires=[],\\n  packages=setuptools.find_packages()\\n)\"\n",
        "! echo \"$setup_py\" > custom/setup.py\n",
        "\n",
        "pkg_info = \"Metadata-Version: 1.0\\n\\nName: PPG to BP Training \\n\\nVersion: 0.0.0\\n\\nSummary: Training Setup\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nLicense: Public\\n\\nDescription: Training Setup\\n\\nPlatform: Vertex\"\n",
        "! echo \"$pkg_info\" > custom/PKG-INFO\n",
        "\n",
        "# Make the training subfolder\n",
        "! mkdir custom/trainer\n",
        "! touch custom/trainer/__init__.py"
      ],
      "metadata": {
        "id": "fTOlutbCJkUl"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add trainer code"
      ],
      "metadata": {
        "id": "gE8vDmMRKz4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile custom/trainer/task.py\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Subtract\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import argparse\n",
        "import pickle\n",
        "from tensorflow.python.lib.io import file_io\n",
        "import sqlite3\n",
        "from trainer.db import Database\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--model_dir', dest='model_dir')\n",
        "parser.add_argument('--bucket_name', dest='bucket_name')\n",
        "parser.add_argument('--user_id', dest='userID')\n",
        "args = parser.parse_args()\n",
        "\n",
        "model_dir = args.model_dir\n",
        "bucket_name = args.bucket_name\n",
        "userID = int(args.userID)\n",
        "\n",
        "basePath = f'/gcs/{bucket_name}/'\n",
        "\n",
        "databasePath = os.path.join(basePath, 'database/calibration_data.db')\n",
        "calibrationDataPath = os.path.join(basePath, f'user{userID}/calibration/')\n",
        "conn = sqlite3.connect(databasePath)\n",
        "db = Database(conn, calibrationDataPath)\n",
        "\n",
        "def getModel():\n",
        "  inputs = layers.Input(shape=(79,79,1))\n",
        "  x = layers.Conv2D(filters=96, kernel_size=(5,5), strides=(2,2), activation='relu')(inputs)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
        "  x = layers.Conv2D(filters=384, kernel_size=(5,5), strides=(1,1), activation='relu', padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
        "  x = layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  inputsFeatures = layers.Input(shape=(1,))\n",
        "  x = layers.Concatenate()([x, inputsFeatures])\n",
        "  x = layers.Dense(4096, activation='relu')(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  x = layers.Dense(4096, activation='relu')(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(2, activation='linear')(x)\n",
        "  model = keras.Model(inputs=[inputs, inputsFeatures], outputs=outputs)\n",
        "  return model\n",
        "\n",
        "def getModelS(model1, model2):\n",
        "  last_layer = model1.layers[-1].output\n",
        "  last_layer1 = model2.layers[-1].output\n",
        "  outputs = Subtract()([last_layer, last_layer1])\n",
        "  modelS = keras.Model(inputs=[model1.inputs, model2.inputs], outputs=outputs)\n",
        "  return modelS\n",
        "\n",
        "def getModelWeights(path, tf=False):\n",
        "  if not tf:\n",
        "    with open(path, 'rb') as file:\n",
        "      weights = pickle.load(file)\n",
        "      weights = weights['weights']\n",
        "      return weights\n",
        "  else:\n",
        "    with file_io.FileIO(path, 'rb') as file:\n",
        "      weights = pickle.load(file)\n",
        "      weights = weights['weights']\n",
        "      return weights\n",
        "\n",
        "class EarlyStoppingByLossVal(Callback):\n",
        "  def __init__(self, monitor='val_loss', value=0.00001, minEpoch=10, v=4, verbose=0):\n",
        "    super(Callback, self).__init__()\n",
        "    self.monitor = monitor\n",
        "    self.value = value\n",
        "    self.minEpoch = minEpoch\n",
        "    self.v = v\n",
        "    self.verbose = verbose\n",
        "    self.prevValue = 1000\n",
        "    self.count = 0\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    current = logs.get(self.monitor)\n",
        "    if current is None:\n",
        "      print(\"Early stopping requires %s available!\" % self.monitor)\n",
        "\n",
        "    if current < self.prevValue or abs(current - self.prevValue) < self.v:\n",
        "      self.prevValue = current\n",
        "      if epoch > self.minEpoch and self.count > 10:\n",
        "        if self.verbose > 0:\n",
        "          print(\"\\nEpoch %05d: early stopping THR\" % epoch)\n",
        "        self.model.stop_training = True\n",
        "      self.count = self.count + 1\n",
        "    else:\n",
        "      self.count = 0\n",
        "\n",
        "def calibrateModel(model, train, trainHR, trainLabels):\n",
        "  model1 = getModel()\n",
        "  model1.build((None, 79, 79))\n",
        "  model1.compile(\n",
        "      optimizer=keras.optimizers.Adam(),\n",
        "      loss=keras.losses.MeanAbsoluteError()\n",
        "  )\n",
        "  model1.set_weights(model.get_weights())\n",
        "\n",
        "  callbacks = [EarlyStoppingByLossVal(monitor='loss', value=8, minEpoch=20, v=8)]\n",
        "\n",
        "  model1.fit(\n",
        "      [train, trainHR],\n",
        "      trainLabels,\n",
        "      epochs=40,\n",
        "      batch_size=1,\n",
        "      callbacks=callbacks,\n",
        "      shuffle=False,\n",
        "      verbose=0\n",
        "  )\n",
        "\n",
        "  model2 = getModel()\n",
        "  model2.build((None, 79, 79))\n",
        "  model2.compile(\n",
        "      optimizer=keras.optimizers.Adam(),\n",
        "      loss=keras.losses.MeanAbsoluteError()\n",
        "  )\n",
        "  model2.set_weights(model1.get_weights())\n",
        "\n",
        "  model1.trainable = False\n",
        "  model2.trainable = False\n",
        "\n",
        "  model4 = getModelS(model1, model2)\n",
        "  return model1, model4\n",
        "\n",
        "def getLabels(SBP, DBP):\n",
        "  labels = [[SBP[i], DBP[i]] for i in range(0, len(SBP))]\n",
        "  labels = np.vstack(labels)\n",
        "  return labels\n",
        "\n",
        "calDataPath = os.path.join(basePath, f'user{userID}/calibration/')\n",
        "# calDataPath = f'gs://{bucket_name}/user{userID}/calibration/'\n",
        "modelWeightsPath = os.path.join(basePath, f'user{userID}/model/model_weightsPPG.pkl')\n",
        "\n",
        "allData = []\n",
        "allHR = []\n",
        "allLabels = []\n",
        "calData = []\n",
        "calHR = []\n",
        "calLabels = []\n",
        "predData = []\n",
        "predHR = []\n",
        "predLabels = []\n",
        "# calDataPaths = file_io.list_directory(calDataPath)\n",
        "calDataPaths = os.listdir(calDataPath)\n",
        "calTypes = ['baseline', 'resting', 'exercise']\n",
        "\n",
        "data = db.getCalData(userID, 'all')\n",
        "allData = []\n",
        "allHR = []\n",
        "allSBP = []\n",
        "allDBP = []\n",
        "calKeys = ['B', 'R', 'E']\n",
        "for key in calKeys:\n",
        "  cal = data[key]\n",
        "  allData += cal['data']\n",
        "  allHR += cal['HR']\n",
        "  allSBP += cal['SBP']\n",
        "  allDBP += cal['DBP']\n",
        "\n",
        "X_trainCal = np.vstack(allData)\n",
        "X_trainHRCal = np.vstack(allHR)\n",
        "Y_trainCal = getLabels(allSBP, allDBP)\n",
        "\n",
        "calRE = data['R']['data'] + data['E']['data']\n",
        "HR = data['R']['HR'] + data['E']['HR']\n",
        "SBP = data['R']['SBP'] + data['E']['SBP']\n",
        "DBP = data['R']['DBP'] + data['E']['DBP']\n",
        "\n",
        "predData = np.vstack(calRE)\n",
        "predHR = np.vstack(HR)\n",
        "predLabels = getLabels(SBP, DBP)\n",
        "\n",
        "calData = np.repeat(data['B']['data'][0], len(predData), axis=0)\n",
        "calHR = np.repeat(data['B']['HR'], len(predHR), axis=0)\n",
        "calLabels = np.repeat([[data['B']['SBP'][0], data['B']['DBP'][0]]], len(predLabels), axis=0)\n",
        "\n",
        "baseModelWeightsPath = os.path.join(basePath, 'custom_prediction_routine_tutorial/model/model_weightsPPG.pkl')\n",
        "# baseModelWeightsPath = f'gs://{bucket_name}/user{userID}/model/model_weightsPPG2.pkl'\n",
        "weights = getModelWeights(baseModelWeightsPath, tf=True)\n",
        "  \n",
        "modelS = getModel()\n",
        "modelS.set_weights(weights)\n",
        "\n",
        "calibrate = True\n",
        "if calibrate:\n",
        "  calibratedModels = []\n",
        "  calibratedValues = []\n",
        "  m = []\n",
        "  for i in range(30):\n",
        "    model1, model4 = calibrateModel(modelS, X_trainCal, X_trainHRCal, Y_trainCal)\n",
        "    # calibratedModels.append(model4)\n",
        "    calibratedModels.append(model1)\n",
        "    prediction = model4.predict([[calData, calHR], [predData, predHR]])\n",
        "    SBP = calLabels[:,0] - prediction[:,0]\n",
        "    DBP = calLabels[:,1] - prediction[:,1]\n",
        "    valueSBP = np.abs(SBP-predLabels[:,0])\n",
        "    valueDBP = np.abs(DBP-predLabels[:,1])\n",
        "    values = np.vstack((valueSBP, valueDBP)).T\n",
        "    value = np.linalg.norm(values)\n",
        "    # predicted = calibrationWindowLabels - prediction\n",
        "    # value = np.abs(predicted - Y_trainCal2)\n",
        "    calibratedValues.append(value)\n",
        "  print(calibratedValues)\n",
        "  minValue = np.argmin(calibratedValues)\n",
        "  # model4 = calibratedModels[minValue]\n",
        "  model1 = calibratedModels[minValue]\n",
        "  calibratedModels = []\n",
        "# else:\n",
        "  # modelS = keras.models.load_model(filePath)\n",
        "# fileName = f\"modelPPG5.h5\"\n",
        "# filePath = f\"/content/drive/MyDrive/Colab Notebooks/{fileName}\"\n",
        "# model1.save(filePath)\n",
        "\n",
        "weights = {\n",
        "  'weights': model1.get_weights()\n",
        "}\n",
        "\n",
        "with open(modelWeightsPath, 'wb') as file:\n",
        "  pickle.dump(weights, file)\n",
        "\n",
        "fileName = os.path.basename(modelWeightsPath)\n",
        "db.insertModelData(userID, fileName)\n",
        "\n",
        "# prediction = model4.predict([[X_trainCal3, X_trainHRCal3], [X_trainCal2, X_trainHRCal2]], verbose=0)\n",
        "# print(prediction)\n",
        "# SBP = calibrationWindowLabels[:,0] - prediction[:,0]\n",
        "# DBP = calibrationWindowLabels[:,1] - prediction[:,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SomKz7qK1q5",
        "outputId": "18ddef2c-f14e-4f00-bdf0-39d6c8768e31"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing custom/trainer/task.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile custom/trainer/db.py\n",
        "import json\n",
        "import pickle\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "class Database(object):\n",
        "  def __init__(self, conn, basePath):\n",
        "    self.conn = conn\n",
        "    self.basePath = basePath\n",
        "  \n",
        "  def getCalData(self, userID, calType):\n",
        "    if calType == 'all':\n",
        "      selectQuery = f\"SELECT HR, SBP, DBP, filename, code FROM calibration_info, cal_codes WHERE userid = {userID} AND calibration_info.calcodeid = cal_codes.id\"\n",
        "      try:\n",
        "        records = self.conn.execute(selectQuery).fetchall()\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        return False\n",
        "      data = {\n",
        "        'B': {\n",
        "          'HR': [],\n",
        "          'SBP': [],\n",
        "          'DBP': [],\n",
        "          'data': []\n",
        "        },\n",
        "        'R': {\n",
        "          'HR': [],\n",
        "          'SBP': [],\n",
        "          'DBP': [],\n",
        "          'data': []\n",
        "        },\n",
        "        'E': {\n",
        "          'HR': [],\n",
        "          'SBP': [],\n",
        "          'DBP': [],\n",
        "          'data': []\n",
        "        }\n",
        "      }\n",
        "      for row in records:\n",
        "        data[row[4]]['HR'].append([row[0]])\n",
        "        data[row[4]]['SBP'].append(row[1])\n",
        "        data[row[4]]['DBP'].append(row[2])\n",
        "        with open(self.basePath + row[3], 'rb') as file:\n",
        "          curData = pickle.load(file)\n",
        "        data[row[4]]['data'].append(curData['data'])  \n",
        "    else:\n",
        "      selectQuery = f\"SELECT HR, SBP, DBP, filename FROM calibration_info WHERE userid = {userID} AND calcodeid = (SELECT id FROM cal_codes WHERE code = \\'{calType}\\')\"\n",
        "      try:\n",
        "        records = self.conn.execute(selectQuery).fetchall()\n",
        "      except Exception as e:\n",
        "        print(e)\n",
        "        return False\n",
        "      data = {\n",
        "        'HR': [],\n",
        "        'SBP': [],\n",
        "        'DBP': [],\n",
        "        'data': []\n",
        "      }\n",
        "      for row in records:\n",
        "        data['HR'].append([row[0]])\n",
        "        data['SBP'].append(row[1])\n",
        "        data['DBP'].append(row[2])\n",
        "        with open(self.basePath + row[3], 'rb') as file:\n",
        "          curData = pickle.load(file)\n",
        "        data['data'].append(curData['data'])\n",
        "    return data\n",
        "  \n",
        "  def insertModelData(self, userID, fileName):\n",
        "    selectQuery = f'SELECT id FROM models WHERE userid = {userID}'\n",
        "    id = self.conn.execute(selectQuery).fetchone()\n",
        "    if id:\n",
        "      id = id[0]\n",
        "      status = self.updateModelData(id)\n",
        "      if status:\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "    insertQuery = f'INSERT INTO models (userid, filename) VALUES ({userID}, \\'{fileName}\\')'\n",
        "    try:\n",
        "      self.conn.executescript(insertQuery)\n",
        "      self.conn.commit()\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return False\n",
        "\n",
        "  def updateModelData(self, id):\n",
        "    updateQuery = f\"UPDATE models SET createtime = datetime('now') WHERE id = {id};\"\n",
        "\n",
        "    try:\n",
        "      self.conn.executescript(updateQuery)\n",
        "      self.conn.commit()\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      return False\n",
        "  \n",
        "    return True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SsFH4SQLAYH",
        "outputId": "53ac8e57-7f80-4bba-da01-7be32919337d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing custom/trainer/db.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dockerfile"
      ],
      "metadata": {
        "id": "vrvqddccLY4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile custom/Dockerfile\n",
        "FROM tensorflow/tensorflow:2.3.4-gpu\n",
        "RUN pip install heartpy pandas keras==2.3.1\n",
        "WORKDIR /root\n",
        "\n",
        "WORKDIR /\n",
        "\n",
        "COPY trainer /trainer\n",
        "\n",
        "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N782GiSRLap9",
        "outputId": "74122968-1662-4bc0-9389-59c67ce0bf73"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing custom/Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build training container"
      ],
      "metadata": {
        "id": "xsqWj6HGLiTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REPOSITORY = \"calibration-train\"\n",
        "CONTAINER_NAME = \"tf-gpu.2-3\"\n",
        "TAG = \"latest\"\n",
        "TRAIN_IMAGE = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{CONTAINER_NAME}:{TAG}\""
      ],
      "metadata": {
        "id": "y0B3UQVzLkQu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if newInstance:\n",
        "  ! gcloud artifacts repositories create {REPOSITORY} --repository-format=docker --location={REGION} --description=\"Docker repository for custom training\"\n",
        "! gcloud artifacts repositories list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccLwRPyuMQNd",
        "outputId": "03ec80e8-93b6-4a76-c6dd-c035207b9214"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing items under project calcium-bot-373723, across all locations.\n",
            "\n",
            "                                                                               ARTIFACT_REGISTRY\n",
            "REPOSITORY         FORMAT  MODE                 DESCRIPTION                            LOCATION  LABELS  ENCRYPTION          CREATE_TIME          UPDATE_TIME          SIZE (MB)\n",
            "calibration-train  DOCKER  STANDARD_REPOSITORY  Docker repository for custom training  us-west4          Google-managed key  2023-03-08T20:59:46  2023-03-21T19:21:11  3260.104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if newInstance:\n",
        "  %cd custom\n",
        "  !gcloud builds submit --region={REGION} --tag=$TRAIN_IMAGE\n",
        "  %cd .."
      ],
      "metadata": {
        "id": "xMlzz-WlM9qu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store training code on Cloud Storage Bucket"
      ],
      "metadata": {
        "id": "jbgsfDfGNIc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -f custom.tar custom.tar.gz\n",
        "! tar cvf custom.tar custom\n",
        "! gzip custom.tar\n",
        "! gsutil cp custom.tar.gz $BUCKET_URI/custom_trainer/trainer_PPGtoBP.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XIPqaRbNNiv",
        "outputId": "0c8c0300-f3b4-4573-b706-35c060ce3ee6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custom/\n",
            "custom/trainer/\n",
            "custom/trainer/task.py\n",
            "custom/trainer/db.py\n",
            "custom/trainer/__init__.py\n",
            "custom/Dockerfile\n",
            "custom/PKG-INFO\n",
            "custom/setup.py\n",
            "custom/setup.cfg\n",
            "custom/README.md\n",
            "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
            "/ [1 files][  3.6 KiB/  3.6 KiB]                                                \n",
            "Operation completed over 1 objects/3.6 KiB.                                      \n"
          ]
        }
      ]
    }
  ]
}